{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mood Prediction Using Smartphone Data\n",
    "### Data Mining Techniques | Vrije Universiteit Amsterdam | April 2024\n",
    "##### Lieve Jilesen (ljn278), Ryan Ott (rot280), and Jaime Perez y Perez (jpz240) | Group 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages & loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# sns.set(style=\"whitegrid\")\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "file_path = 'data/dataset_mood_smartphone.csv'\n",
    "df = pd.read_csv(file_path, parse_dates=['time'], index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = df.shape[0]\n",
    "num_participants = df.id.nunique()\n",
    "num_days = df.time.dt.date.nunique()\n",
    "num_variables = df.variable.nunique()\n",
    "\n",
    "data_characteristics = {\n",
    "    \"Number of Records\": [num_records],\n",
    "    \"Number of Participants\": [num_participants],\n",
    "    \"Number of Days\": [num_days],\n",
    "    \"Number of Variables\": [num_variables],\n",
    "}\n",
    "\n",
    "data_characteristics_df = pd.DataFrame(data_characteristics)\n",
    "data_characteristics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for the dataset\n",
    "summary_data = {\n",
    "    \"Variable\": [],\n",
    "    \"Data Type\": [],\n",
    "    \"Total Records\": [],\n",
    "    \"Unique Values\": [],\n",
    "    \"Missing Values\": [],\n",
    "    \"Mean\": [],\n",
    "    \"Std\": [],\n",
    "    \"Min\": [],\n",
    "    \"25%\": [],\n",
    "    \"50%\": [],\n",
    "    \"75%\": [],\n",
    "    \"Max\": []\n",
    "}\n",
    "\n",
    "for var in df.variable.unique():\n",
    "    var_data = df[df['variable'] == var]['value']\n",
    "    desc = var_data.describe()\n",
    "    summary_data[\"Variable\"].append(var)\n",
    "    summary_data[\"Data Type\"].append(df[df['variable'] == var]['value'].dtype)\n",
    "    summary_data[\"Total Records\"].append(var_data.count())\n",
    "    summary_data[\"Unique Values\"].append(var_data.nunique())\n",
    "    summary_data[\"Missing Values\"].append(df[df['variable'] == var]['value'].isnull().sum())\n",
    "    summary_data[\"Mean\"].append(desc['mean'])\n",
    "    summary_data[\"Std\"].append(desc['std'])\n",
    "    summary_data[\"Min\"].append(desc['min'])\n",
    "    summary_data[\"25%\"].append(desc['25%'])\n",
    "    summary_data[\"50%\"].append(desc['50%'])\n",
    "    summary_data[\"75%\"].append(desc['75%'])\n",
    "    summary_data[\"Max\"].append(desc['max'])\n",
    "\n",
    "summary_table = pd.DataFrame(summary_data)\n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any records missing id or time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have any records without index, id or time?\n",
    "df[df['id'].isnull() | df['time'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, thankfully not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of variable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [var for var in df['variable'].unique() if var not in ['sms', 'call']]\n",
    "fig, axes = plt.subplots(nrows=len(variables), ncols=1, figsize=(10, 6 * len(variables)))\n",
    "\n",
    "for ax, var in zip(axes.flatten(), variables):\n",
    "    var_data = df[df['variable'] == var]['value'].dropna()\n",
    "    mean = var_data.mean()\n",
    "    std = var_data.std()\n",
    "    # Adjust bins for better visualization based on data range and characteristics\n",
    "    bins = min(30, int(var_data.nunique()))  # Use a minimum of 30 bins or less if fewer unique values\n",
    "\n",
    "    ax.hist(var_data, bins=bins, alpha=0.75, color='blue', edgecolor='black', label=f'{var} Scores')\n",
    "    ax.set_title(f'Distribution of {var} values')\n",
    "    ax.set_xlabel(f'{var.capitalize()} Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(axis='y', alpha=0.75)\n",
    "    legend_label = f\"Mean: {mean:.2f}, Std: {std:.2f}\"\n",
    "    ax.legend([f\"{var.capitalize()} Scores\\n{legend_label}\"], loc='upper right', title='Statistics', frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram showing the distribution of possible mood scores in our dataset. The mood of participants seems to be normally distributed with a mean and standard deviation of around 7 and 1 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mood for a participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series of Mood for a Selected Participant\n",
    "participant_data = df[(df['id'] == 'AS14.01') & (df['variable'] == 'mood')]\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(participant_data['time'], participant_data['value'], marker='o', linestyle='-')\n",
    "plt.title('Mood Over Time for Participant AS14.01')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mood Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries plot of the mood scores given by the first participant. We see it being around 7 +- 1 for most of the time, with rare moments of extreme emotions. We also see long gaps in the record availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation map for variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of variables\n",
    "pivot_table = df.pivot_table(index=['id', 'time'], columns='variable', values='value')\n",
    "correlation_matrix = pivot_table.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation heatmap to visualize the correlation between different variables in the dataset. The heatmap provides a color-coded representation of the correlation values, with warmer colors indicating stronger positive correlations and cooler colors indicating stronger negative correlations. (Call is not visible as its boolean and only recorded when a call takes place so it always has a value of 1).\n",
    "\n",
    "Arousal, and especially valence seem to be highly positively corellated with mood for example, while using finance apps seems to highly negatively correlate with screen time, which could be useful features to consider when making new predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity & mood relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity and Mood Relationship\n",
    "activity_data = df[df['variable'] == 'activity']\n",
    "mood_data = df[df['variable'] == 'mood']\n",
    "\n",
    "# Merging on ID and Time for direct comparison\n",
    "activity_mood_merged = pd.merge(activity_data, mood_data, on=['id', 'time'], suffixes=('_activity', '_mood'))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(activity_mood_merged['value_mood'], activity_mood_merged['value_activity'], alpha=0.5)\n",
    "plt.title('Mood vs. Activity')\n",
    "plt.xlabel('Mood Score')\n",
    "plt.ylabel('Activity Score')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot showing co-occurence of mood with activiy scores. If the mood is in the common range of 7Â±1 we see participants record a wide range of activity scores, while if the mood is low, there is no high activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Data Cleaning (Still w.i.p. ryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because call and sms are boolean (only present if true) we convert them to boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'call' and 'sms' data to boolean\n",
    "for variable in ['call', 'sms']:\n",
    "    df.loc[df['variable'] == variable, 'value'] = df[df['variable'] == variable]['value'].fillna(0).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing incorrect values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all `appCat.*` variables and `screen` should denote a time duration, they cannot be negative. Thus, any records with negative duration must be incorrect and removed. We see this is the case for example when looking at the variables properties table under section 1.1, where the min value for `appCat.builtin` is highly negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables denoting time durations\n",
    "time_variables = [var for var in df['variable'].unique() if 'appCat.' in var or var == 'screen']\n",
    "\n",
    "# Remove records with negative durations for time variables\n",
    "print(\"Removed records due to negative values:\")\n",
    "initial_count = df.shape[0]\n",
    "for var in time_variables:\n",
    "    before_count = df[df['variable'] == var].shape[0]\n",
    "    df = df[~((df['variable'] == var) & (df['value'] < 0))]\n",
    "    after_count = df[df['variable'] == var].shape[0]\n",
    "    print(f\"{before_count - after_count} from {var}\")\n",
    "\n",
    "total_removed = initial_count - df.shape[0]\n",
    "print(f\"Total records removed: {total_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing extreme values (Todo, Ryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numeric variables that are not self-reported by the participants (so not mood, arousal, valence & activity, and also not call or SMS) we remove values that lie outside of the z-score threshold of Â±3. Points that are more than 3 standard deviations away from the mean are considered outliers and should thus not be considered in our dataset. This should offer a more robust removal of outliers than other statistical methods like Interquartile Range, because it can better handle data distributions with heavier tails, like some of our variable values are as can be seen by the histograms for section 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for which we remove outliers\n",
    "exclude_vars = ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'call', 'sms']\n",
    "outlier_vars = [var for var in df['variable'].unique() if var not in exclude_vars]\n",
    "\n",
    "# Removing outliers using Z-score method\n",
    "initial_count = df.shape[0]\n",
    "for var in outlier_vars:\n",
    "    # Calculate Z-scores for the variable\n",
    "    var_data = df[df['variable'] == var]['value']\n",
    "    z_scores = np.abs((var_data - var_data.mean()) / var_data.std())\n",
    "    \n",
    "    # Filter out data points where the absolute Z-score is greater than 3\n",
    "    before_count = df[df['variable'] == var].shape[0]\n",
    "    df = df[~((df['variable'] == var) & (z_scores > 3))]\n",
    "    after_count = df[df['variable'] == var].shape[0]\n",
    "    print(f'Removed {before_count - after_count} outliers from {var}')\n",
    "\n",
    "total_removed = initial_count - df.shape[0]\n",
    "print(f'Total records removed: {total_removed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of variable values post cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [var for var in df['variable'].unique() if var not in ['sms', 'call']]\n",
    "fig, axes = plt.subplots(nrows=len(variables), ncols=1, figsize=(10, 6 * len(variables)))\n",
    "\n",
    "for ax, var in zip(axes.flatten(), variables):\n",
    "    var_data = df[df['variable'] == var]['value'].dropna()\n",
    "    mean = var_data.mean()\n",
    "    std = var_data.std()\n",
    "    # Adjust bins for better visualization based on data range and characteristics\n",
    "    bins = min(30, int(var_data.nunique()))  # Use a minimum of 30 bins or less if fewer unique values\n",
    "\n",
    "    ax.hist(var_data, bins=bins, alpha=0.75, color='blue', edgecolor='black', label=f'{var} Scores')\n",
    "    ax.set_title(f'Distribution of {var} values')\n",
    "    ax.set_xlabel(f'{var.capitalize()} Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(axis='y', alpha=0.75)\n",
    "    legend_label = f\"Mean: {mean:.2f}, Std: {std:.2f}\"\n",
    "    ax.legend([f\"{var.capitalize()} Scores\\n{legend_label}\"], loc='upper right', title='Statistics', frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that its much better......todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values (Todo, Ryan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start on basic predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [374651, 5641]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/DMT-Group17/Assignment1/mood_prediction.ipynb Cell 41\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bcongenial-memory-qjv6wvr9jw24gqx/workspaces/DMT-Group17/Assignment1/mood_prediction.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m x \u001b[39m=\u001b[39m df \u001b[39m# input features\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-memory-qjv6wvr9jw24gqx/workspaces/DMT-Group17/Assignment1/mood_prediction.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m y \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mvariable\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmood\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# target\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bcongenial-memory-qjv6wvr9jw24gqx/workspaces/DMT-Group17/Assignment1/mood_prediction.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m x_train, x_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(x, y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-memory-qjv6wvr9jw24gqx/workspaces/DMT-Group17/Assignment1/mood_prediction.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Transform data \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcongenial-memory-qjv6wvr9jw24gqx/workspaces/DMT-Group17/Assignment1/mood_prediction.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2657\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2654\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2657\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[1;32m   2659\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m   2660\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2661\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[1;32m   2662\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 514\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[1;32m    515\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [374651, 5641]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# NOTE: this is still little basic format for the model, will maybe have to run on Google colab?\n",
    "\n",
    "\n",
    "x = df # input features\n",
    "y = df[df['variable'] == 'mood'] # target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform data \n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "x_train_reshaped = x_train_scaled.reshape(x_train_scaled.shape[0], x_train_scaled.shape[1], 1)\n",
    "x_test_reshaped = x_test_scaled.reshape(x_test_scaled.shape[0], x_test_scaled.shape[1], 1)\n",
    "\n",
    "# Define the CNN model, three layers\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(x_train_reshaped.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(x_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(x_test_reshaped, y_test))\n",
    "\n",
    "loss = model.evaluate(x_test_reshaped, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "predictions = model.predict(x_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code (I think Lieve's :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data by dropping NA values\n",
    "\n",
    "cleaned_data = df[df['value'] != 'NA'].dropna()\n",
    "\n",
    "# TODO: remove outliers for each variable and write in report why/how\n",
    "\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of mood \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(cleaned_data[cleaned_data['variable'] == 'mood']['value'], bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Mood')\n",
    "plt.xlabel('Mood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Boxplot of arousal \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(cleaned_data[cleaned_data['variable'] == 'circumplex.arousal']['value'], vert=True)\n",
    "plt.title('Boxplot of arousal')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of valence \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(cleaned_data[cleaned_data['variable'] == 'circumplex.valence']['value'], vert=True)\n",
    "plt.title('Boxplot of valence')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series plot of mood\n",
    "# TODO: fix errors and readability of plot\n",
    "grouped_data = cleaned_data.groupby('id')\n",
    "\n",
    "# Plot for each 'id'\n",
    "plt.figure(figsize=(30, 6))\n",
    "for name, group in grouped_data:\n",
    "    plt.plot(group[cleaned_data['variable'] == 'mood']['time'], group[cleaned_data['variable'] == 'mood']['value'], label=name, linewidth=1)\n",
    "    # plt.plot(group['time'], group['value'], label=name, linewidth=1)\n",
    "    # plt.plot(cleaned_data[cleaned_data['variable'] == 'mood']['time'], cleaned_data[cleaned_data['variable'] == 'mood']['value'], linewidth=1)\n",
    "\n",
    "plt.title('Time Series Plot of Mood')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mood')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Time series of activity\n",
    "# TODO: fix errors and readability of plot\n",
    "grouped_data = cleaned_data.groupby('id')\n",
    "\n",
    "# Plot for each 'id'\n",
    "plt.figure(figsize=(30, 6))\n",
    "for name, group in grouped_data:\n",
    "    plt.plot(group[cleaned_data['variable'] == 'activity']['time'], group[cleaned_data['variable'] == 'activity']['value'], label=name, linewidth=1)\n",
    "    # plt.plot(group['time'], group['value'], label=name, linewidth=1)\n",
    "    # plt.plot(cleaned_data[cleaned_data['variable'] == 'mood']['time'], cleaned_data[cleaned_data['variable'] == 'mood']['value'], linewidth=1)\n",
    "\n",
    "plt.title('Time Series Plot of Activity')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Activity')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series of activity AND mood\n",
    "\n",
    "avg_activity = cleaned_data[cleaned_data['variable'] == 'activity'].groupby(cleaned_data['time'].dt.date)['value'].mean()\n",
    "avg_mood = cleaned_data[cleaned_data['variable'] == 'mood'].groupby(cleaned_data['time'].dt.date)['value'].mean()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Activity', color='tab:red')\n",
    "ax1.plot(avg_activity.index, avg_activity.values, color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "ax2.set_ylabel('Mood', color='tab:blue')\n",
    "ax2.plot(avg_mood.index, avg_mood.values, color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "# Time series of screen AND mood\n",
    "avg_screen = cleaned_data[cleaned_data['variable'] == 'screen'].groupby(cleaned_data['time'].dt.date)['value'].mean()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('screen', color='tab:red')\n",
    "ax1.plot(avg_screen.index, avg_screen.values, color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "ax2.set_ylabel('Mood', color='tab:blue')\n",
    "ax2.plot(avg_mood.index, avg_mood.values, color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "# Time series of utilities app and mood\n",
    "avg_ut = cleaned_data[cleaned_data['variable'] == 'appCat.utilities'].groupby(cleaned_data['time'].dt.date)['value'].mean()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Utilities', color='tab:red')\n",
    "ax1.plot(avg_ut.index, avg_ut.values, color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "ax2.set_ylabel('Mood', color='tab:blue')\n",
    "ax2.plot(avg_mood.index, avg_mood.values, color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "df = pd.DataFrame(cleaned_data, columns=[\"\", \"id\", \"time\", \"variable\", \"value\"])\n",
    "\n",
    "groups = df.groupby('variable')['value']\n",
    "\n",
    "# TODO: fix statistical test so we can remove variables which are not significant\n",
    "\n",
    "# Perform statistical tests for each variable\n",
    "for var, group in groups:\n",
    "    if var != 'mood':\n",
    "        if len(group.unique()) > 2: \n",
    "            f_statistic, p_value = stats.f_oneway(*[group for name, group in groups])\n",
    "            if p_value < 0.05:\n",
    "                print(f\"Variable '{var}' is significant (p-value: {p_value:.4f}, f stat: {f_statistic})\")\n",
    "            else:\n",
    "                print(f\"Variable '{var}' is not significant (p-value: {p_value:.4f}, f stat: {f_statistic})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
